# Домашняя работа №1

За основу взят датасет и соревнование [Criteo Display Advertising challenge](https://www.kaggle.com/c/criteo-display-ad-challenge/overview)

## Задание 

Предскажите вероятность клика на рекламный баннер. Признаки включают в себя некоторое количество целочисленных и категориальных переменных. Данные в обучающем датасете покрывают 7 последовательных дней, данные валидационного датасета покрывают 8-ой день. 

Первая колонка датасета - идентификатор записи - целое число.
Вторая - целевая переменная: 1 - клик, 0 - нет клика
Следующие 13 - целочисленные признаки, имеются пропуски.
Следующие 26 - категориальные признаки, так же имеются пропуски
Последняя колонка - порядковый номер дня. Заметим что нет никакой информации о днях недели, которым соответствуют номера дней в датасетах.

Для простоты, возьмите следующее определение полей датасета:

```
numeric_features = ["if"+str(i) for i in range(1,14)]
categorical_features = ["cf"+str(i) for i in range(1,27)] + ["day_number"]

fields = ["id", "label"] + numeric_features + categorical_features
```

Метрика - [Log loss](<a href="http://wiki.fast.ai/index.php/Log_Loss">http://wiki.fast.ai/index.php/Log_Loss</a>).

Более подробное описание задачи можно найти на сайте соревнования по ссылке выше.
Заметьте, что в соревновании на Kaggle в датасете не было поля day_number, оно было добавлено при семплировании из расширенного датасета Criteo, в котором каждый файл данных содержал один день. Вы можете использовать day_number или нет, на ваше усмотрение.

Задание очень похоже на примерное задание https://github.com/datamove/ozon-masters-bigdata/blob/master/projects/tut1/README.md

Однако просто скопировать весь код не удастся, надо будет немного поработать над ним.

Вы можете пользоваться моделями, описанными на форуме соревнования, искать среди них идеи, но ваше решение все равно должно укладываться в заданные рамки оформления работы, см. ниже. Мы не устраиваем соревнования, и поэтому вам не стоит прилагать лишних усилий по улучшению метрики.

## Условие прохождения

Превышение baseline метрики (0.5) на тестовом датасете.

## Dataset

Обратите внимание, что разделитель в этих датасетах - TAB. Сохраняйте этот разделитель на этапах фильтрации и предсказания.

### Обучающий

Обучающий датасет расположен в локальной файловой системе на вашем login-узле:

`/home/users/datasets/criteo/criteo_train1`

### Валидационный датасет для работы со срезами.

Это большой, на 20 Гб датасет, на котором можно применять условия фильтрации. 

HDFS: /datasets/criteo_valid_large_features

Истинные значения для среза: 

HDFS: /datasets/criteo_valid_large_labels
Local: /home/users/datasets/criteo/criteo_valid_large_labels

Имейте в виду, что валидационном датасете вырезана вторая колонка ('label').

Для отладки вы можете взять небольшой сэмпл этого датасета. Напишите map-reduce задачу для этого.

### Условие среза

Условие для реализации в функции filter_cond.py(см. ниже) таково:

Значение в поле if1 (первое числовое поле) таково, что 20 < if1 < 40.

## Оформление работы

### Модель

Разработайте модель в виде пайплайна и сохраните ее определение в отдельном файле `projects/1/model.py`. Объект пайплайна модели должен называться `model`, например:

```
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('linearregression', LinearRegression())
])
```

и тогда ее можно будет импортировать в других программах как:

`from model import model`

Также в файле с моделью мы определяем поля датасета:

```
numeric_features = ["if"+str(i) for i in range(1,14)]
categorical_features = ["cf"+str(i) for i in range(1,27)] + ["day_number"]

fields = ["id", "label"] + numeric_features + categorical_features
```

тогда их тоже можно будет импортировать подобным образом:

`from model import fields`

### Training

Обучение модели проводится на обучающем датасете на одном узле. Разработайте программу train.py, которая импортирует ранее определенную модель и обучает её. На вход в качестве аргумента подается

* номер проекта
* путь к файлу с обучающей выборкой.

Обученная модель сохраняется в файл 1.joblib используя сериализатор joblib:

```
from joblib import dump
dump(model, "1.joblib")
```

### Запуск обучения

Напишите shell-wrapper для train.py, который будет запускаться следующим образом:

```
cd ozon-masters-bigdata
projects/1/train.sh 1 /path/to/training/dataset
```

где 1 - номер проекта, `/path/to/training/dataset` - путь к файлу с тренировочной выборкой.

### Предсказания (инференс)

Напишите программу predict.py, которая загружает обученную и сохраненную ранее модель:

```
from joblib import load
model = load("1.joblib")
```

и выдает предсказания на стандартный вывод для тестовых записей, которые подаются на стандартный ввод.

### Запуск инференса

Напишите shell-wrapper predict.sh для запуска инференса на валидационном датасете как map-reduce задачи:

```
cd ozon-masters-bigdata
#remove output dataset if exists
hdfs dfs -rm -r -f -skipTrash predicted.csv
projects/1/predict.sh projects/1/predict.py,1.joblib,projects/1/model.py /datasets/criteo_valid_large_features predicted.csv predict.py
```

где параметры:

* файлы для посылки с задачей (включая тренированную модель)
* путь к валидационному датасету 
* путь к файлу с предсказаниями
* скрипт для запуска

### Фильтрация датасета

Разработайте скрипт filter.py для фильтрации датасета, который берет записи из датасета на стандартном входе, применяет некоторую функцию фильтрации и выводит записи, прошедшие фильтр, на стандартный вывод.

Фильтрующая функция определяется в файле filter_cond.py и имеет следующий интерфейс:

```
def filter_cond(line_dict):
    """Filter function
    Takes a dict with field names and values as the argument
    Returns True if conditions are satisfied
    """
    cond_match = (
       int(line_dict["num_reviews"]) > 20
    ) 
    return True if cond_match else False
```

### Запуск фильтрации

Разработайте filter.sh, который должен запускать map-reduce задачу на кластер:

Параметры скрипта filter.sh:

* файлы, которые надо послать вместе с задачей, через запятую
* путь к входному файлу
* путь к выходному файлу
* имя файла с программой маппером, то есть filter.py

### Предсказания на срезе

Выше мы запускали отдельно фильтрацию и предсказания. Теперь мы запустим одну mapreduce задачу в которой мы будем фильтровать датасет на стадии map и предсказывать на стадии reduce. Преимущество - всего одна задача и не надо управлять промежуточными данными.

```
projects/1/filter_predict.sh projects/1/filter.py,projects/1/predict.py,projects/1/filter_cond.py,1.joblib,projects/1/model.py /datasets/criteo_valid_large_features pred_with_filter filter.py predict.py
```

где аргументы:

* файлы, которые надо послать вместе с задачей, через запятую
* путь к входному файлу
* путь к выходному файлу
* имя файла с программой маппера, то есть filter.py.
* имя файла с программой редьюсера, то есть predict.py.

### Расчет метрики

Для расчета метрики можете скопировать файл с предсказаниями в домашнюю директорию и запустить скрипт c нужной функцией из пакета sklearn. См. файл local_scorer.py


## Проверка

Проверка будет осуществляться чекером путем запуска ваших скриптов на отдельной тестовой выборке с тем же условием фильтрации.

Запустите чекер:

`checker.sh 1`

