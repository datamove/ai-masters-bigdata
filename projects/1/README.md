# Домашняя работа №1

За основу взят датасет и соревнование [Criteo Display Advertising challenge](https://www.kaggle.com/c/criteo-display-ad-challenge/overview)

## Задание 

Предскажите вероятность клика на рекламный баннер. Признаки включают в себя некоторое количество целочисленных и категориальных переменных. Данные в обучающем датасете покрывают 7 последовательных дней, данные валидационного датасета покрывают 8-ой день. 

Первая колонка датасета - идентификатор записи - целое число.
Вторая - целевая переменная: 1 - клик, 0 - нет клика
Следующие 13 - целочисленные признаки, имеются пропуски.
Следующие 26 - категориальные признаки, так же имеются пропуски
Последняя колонка - порядковый номер дня. Заметим что нет никакой информации о днях недели, которым соответствуют номера дней в датасетах.

Для простоты, возьмите следующее определение полей датасета:

```
numeric_features = ["if"+str(i) for i in range(1,14)]
categorical_features = ["cf"+str(i) for i in range(1,27)] + ["day_number"]

fields = ["id", "label"] + numeric_features + categorical_features
```

Метрика - [Log loss](<a href="http://wiki.fast.ai/index.php/Log_Loss">http://wiki.fast.ai/index.php/Log_Loss</a>).

Более подробное описание задачи можно найти на сайте соревнования по ссылке выше.
Заметьте, что в соревновании на Kaggle в датасете не было поля day_number, оно было добавлено при семплировании из расширенного датасета Criteo, в котором файлы данных содержали один день. Вы можете использовать day_number на ваше усмотрение.

Задание очень похоже на примерное задание https://github.com/datamove/ozon-masters-bigdata/blob/master/projects/0/README.md

Однако просто скопировать весь код не удастся, надо будет немного поработать над ним.

Вы можете пользоваться моделями, описанными на форуме соревнования, искать среди них идеи, но ваше решение все равно должно укладываться в заданные рамки оформления работы, см. ниже.

## Условие прохождения

Превышение baseline метрики (0.7) на валидационном датасете.

## Dataset

### Обучающий

Обучающий датасет расположен в локальной файловой системе на вашем login-узле:

`/home/users/datasets/criteo_train1`

### Валидационный

Валидационный датасет расположен в HDFS по адресу:

`/datasets/criteo_valid1_features`

Истинные значения валидационного датасета:

В HDFS: `/datasets/criteo_valid1_labels`

в локальной директории: `/datasets/criteo_valid1_labels`

Имейте в виду, что валидационном датасете вырезана вторая колонка ('label').

### Валидационный датасет для работы со срезами.

Это большой, на 20 Гб датасет, на котором можно применять условия фильтрации. 

HDFS: /datasets/criteo_valid_large_features

Истинные значения для среза: 

HDFS: /datasets/criteo_valid_large_labels
Local: /home/users/datasets/criteo_valid1_labels

### Условие среза

Условие для реализации в функции filter_cond.py(см. ниже) таково:

Значение в поле if1 (первое числовое поле) таково, что 20 < if1 < 40.

## Оформление работы

### Модель

Разработайте модель в виде пайплайна и сохраните ее определение в отдельном файле `projects/1/model.py`. Объект пайплайна модели должен называться `model`, например:

```
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('linearregression', LinearRegression())
])
```

и тогда ее можно будет импортировать в других программах как:

`from model import model`

Также в файле с моделью мы определяем поля датасета:

```
numeric_features = ["if"+str(i) for i in range(1,14)]
categorical_features = ["cf"+str(i) for i in range(1,27)] + ["day_number"]

fields = ["id", "label"] + numeric_features + categorical_features
```

тогда их тоже можно будет импортировать подобным образом:

`from model import fields`

### Training

Обучение модели проводится на обучающем датасете на одном узле. Разработайте программу train.py, которая импортирует ранее определенную модель и обучает её. На вход в качестве аргумента подается

* номер проекта
* путь к файлу с обучающей выборкой.

Обученная модель сохраняется в файл 1.joblib используя сериализатор joblib:

```
from joblib import dump
dump(model, "0.joblib")
```

### Запуск обучения

Напишите shell-wrapper для train.py, который будет запускаться следующим образом:

```
cd ozon-masters-bigdata
projects/1/train.sh 1 /path/to/training/dataset
```

где 0 - номер проекта, `/path/to/training/dataset` - путь к файлу с тренировочной выборкой.

### Предсказания (инференс)

Напишите программу predict.py, которая загружает обученную модель и сохраненную ранее модель:

```
from joblib import load
model = load("1.joblib")
```

и выдает предсказания на стандартный вывод для тестовых записей, которые подаются на стандартный ввод.

### Запуск инференса

Напишите shell-wrapper predict.sh для запуска инференса на валидационном датасете как map-reduce задачи:

```
cd ozon-masters-bigdata
#remove output dataset if exists
hdfs dfs -rm -r -f -skipTrash predicted.csv
projects/1/predict.sh projects/1/predict.py,1.joblib /datasets/criteo_valid1_features predicted.csv predict.py
```

где параметры:

* файлы для посылки с задачей (включая тренированную модель)
* путь к валидационному датасету 
* путь к файлу с предсказаниями
* скрипт для запуска

### Фильтрация датасета

Разработайте скрипт filter.py для фильтрации датасета, который берет записи из датасета на стандартном входе, применяет некоторую функцию фильтрации и выводит записи, прошедшие фильтр, на стандартный вывод.

Фильтрующая функция определяется в файле filter_cond.py и имеет следующий интерфейс:

```
def filter_cond(line_dict):
    """Filter function
    Takes a dict with field names and values as the argument
    Returns True if conditions are satisfied
    """
    cond_match = (
       int(line_dict["num_reviews"]) > 20
    ) 
    return True if cond_match else False
```

### Запуск фильтрации

Разработайте filter.sh, который должен запускать map-reduce задачу на кластер:

Параметры скрипта filter.sh:

* файлы, которые надо послать вместе с задачей, через запятую
* путь к входному файлу
* путь к выходному файлу
* имя файла с программой маппером, то есть filter.py

### Предсказания на срезе

Выше мы запускали отдельно фильтрацию и предсказания. Теперь мы запустим одну mapreduce задачу в которой мы будем фильтровать датасет на стадии map и предсказывать на стадии reduce. Преимущество - всего одна задача и не надо управлять промежуточными данными.

```
projects/1/filter_predict.sh projects/1/filter.py,projects/1/predict.py,projects/1/filter_cond.py,1.joblib,projects/1/model.py criteo_valid_large pred_with_filter filter.py predict.py
```

где аргументы:

* файлы, которые надо послать вместе с задачей, через запятую
* путь к входному файлу
* путь к выходному файлу
* имя файла с программой маппера, то есть filter.py.
* имя файла с программой редьюсера, то есть predict.py.


## Проверка

Проверка будет осуществляться на невиденной ранее выборке с неизвестным условием фильтрации.

Запустите чекер:

`ozonm_checker 1`

